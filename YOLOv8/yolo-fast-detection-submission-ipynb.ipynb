{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91249,"databundleVersionId":11294684,"sourceType":"competition"},{"sourceId":11986780,"sourceType":"datasetVersion","datasetId":7539261},{"sourceId":226864880,"sourceType":"kernelVersion"},{"sourceId":422471,"sourceType":"modelInstanceVersion","modelInstanceId":343572,"modelId":364850},{"sourceId":423334,"sourceType":"modelInstanceVersion","modelInstanceId":344070,"modelId":365339}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1473.740735,"end_time":"2025-03-07T10:48:57.252544","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-07T10:24:23.511809","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"1d01bb38f33949f4b3bcc9c2f0762970":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20567e253ea049928561984752c475c3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25d66eddbefd4b789f08c71c15bed03c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4cf768c1fde74a0dbc682af5dccdb007","placeholder":"​","style":"IPY_MODEL_e6a0e7f2fa1448e98ed5468106f38b0e","tabbable":null,"tooltip":null,"value":" 88/88 [00:33&lt;00:00,  2.50it/s]"}},"313db8b650284549aa83576bebc05c49":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51b9f4518c804d3aa60d34fb6125c598","IPY_MODEL_ee1c5c9b87974a71924f3158b582b7bd","IPY_MODEL_25d66eddbefd4b789f08c71c15bed03c"],"layout":"IPY_MODEL_830753d9125a4043bb0fbc711b6cf713","tabbable":null,"tooltip":null}},"4570564545c84099918f4ad9c7c8db03":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cf768c1fde74a0dbc682af5dccdb007":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51b9f4518c804d3aa60d34fb6125c598":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1d01bb38f33949f4b3bcc9c2f0762970","placeholder":"​","style":"IPY_MODEL_85989e1bb95f46ddadbd0dba9ace11f9","tabbable":null,"tooltip":null,"value":"Processing validation motors: 100%"}},"55a44828a5d84ad2afd7e0ee7bc3ad62":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61db5c9867914cf5a03b9d3d2ead8d58":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_20567e253ea049928561984752c475c3","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2fcd0b3560c49d69621ef614d9ac510","tabbable":null,"tooltip":null,"value":363}},"6c7863ad740946cd859316be92b0c1ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7c2cffdbf7141569a93dd9084c1002f","IPY_MODEL_61db5c9867914cf5a03b9d3d2ead8d58","IPY_MODEL_c335430ef0334982b21e57e95dbde1a3"],"layout":"IPY_MODEL_9665a1536c114f33bfc90ec9bebd5459","tabbable":null,"tooltip":null}},"7484c939e31f4cefbd2c5180102c81bf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830753d9125a4043bb0fbc711b6cf713":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85989e1bb95f46ddadbd0dba9ace11f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"87e0b7c76c6440679a4fc099d5604b21":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8fd915718f3248dba9e3d1db811b564e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9665a1536c114f33bfc90ec9bebd5459":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e218bac1bc44058875de1e0c96f7dab":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b2fcd0b3560c49d69621ef614d9ac510":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7c2cffdbf7141569a93dd9084c1002f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4570564545c84099918f4ad9c7c8db03","placeholder":"​","style":"IPY_MODEL_87e0b7c76c6440679a4fc099d5604b21","tabbable":null,"tooltip":null,"value":"Processing training motors: 100%"}},"c335430ef0334982b21e57e95dbde1a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_8fd915718f3248dba9e3d1db811b564e","placeholder":"​","style":"IPY_MODEL_9e218bac1bc44058875de1e0c96f7dab","tabbable":null,"tooltip":null,"value":" 363/363 [02:18&lt;00:00,  2.61it/s]"}},"e6a0e7f2fa1448e98ed5468106f38b0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ee1c5c9b87974a71924f3158b582b7bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_7484c939e31f4cefbd2c5180102c81bf","max":88,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55a44828a5d84ad2afd7e0ee7bc3ad62","tabbable":null,"tooltip":null,"value":88}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Inference Pipeline Overview\n\nThis notebook runs YOLO-based detection of bacterial motors in tomograms with Test-Time Augmentation (TTA).\n\n**Main steps:**\n1. **Setup & Configuration** – Install dependencies, set paths, configure device and parameters.\n2. **Utility Functions** – Profiling, normalization, and image loading helpers.\n3. **TTA Inference** – Run YOLO with original, horizontal flip, and vertical flip; map detections back to original coordinates.\n4. **3D NMS & Processing** – Merge detections across slices into final 3D predictions.\n5. **Submission Generation** – Process all tomograms, apply TTA + 3D NMS, and save results to `submission.csv`.\n6. **Main Execution** – Run the full pipeline and report runtime.","metadata":{}},{"cell_type":"markdown","source":"# Part 1 — Setup, Installs, Imports & Global Config","metadata":{}},{"cell_type":"code","source":"# =========================================\n# Part 1 — Setup, Installs, Imports & Global Config\n# =========================================\n\n# (Kaggle) Optional installs - keep as comments or enable if needed.\n# !tar xfvz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n# !pip install --no-index --find-links=./packages ultralytics\n!pip install -q plotly scikit-learn\n!rm -rf ./packages\n!pip install -q /kaggle/input/ultralytics-timm/ultralytics-8.3.133-py3-none-any.whl --no-deps\n\nimport os, time, threading, random\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import nullcontext\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport torch\nfrom ultralytics import YOLO\n\n# ---- Paths & output ----\nDATA_DIR = '/kaggle/input/byu-locating-bacterial-flagellar-motors-2025'\nTRAIN_CSV = os.path.join(DATA_DIR, 'train_labels.csv')\nTRAIN_DIR = os.path.join(DATA_DIR, 'train')\nTEST_DIR  = os.path.join(DATA_DIR, 'test')\nOUTPUT_DIR = './'\nMODEL_DIR  = './models'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(MODEL_DIR, exist_ok=True)\n\n# ---- Device & seeds ----\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(RANDOM_SEED)\n    torch.backends.cudnn.deterministic = True\n\n# (Some duplication kept from original for safety)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# ---- Submission & model paths ----\ndata_path       = DATA_DIR\ntest_dir        = os.path.join(data_path, \"test\")\nsubmission_path = \"/kaggle/working/submission.csv\"\nmodel_path      = \"/kaggle/input/yolov10b_trust2/pytorch/default/3/best_10m_add_new_dataset.pt\"\n\n# ---- Inference params ----\nCONFIDENCE_THRESHOLD   = 0.45\nMAX_DETECTIONS_PER_TOMO = 3\nNMS_IOU_THRESHOLD      = 0.2\nCONCENTRATION          = 1  # Fraction of slices to process (1 = all)\n\n# ---- TTA configuration ----\nENABLE_TTA = True\n# Supported modes: 'orig', 'hflip', 'vflip'\nTTA_MODES = ['orig', 'hflip', 'vflip']\n\n# ---- Dynamic batch sizing (kept for future use; TTA runs per image) ----\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8\nif device.startswith('cuda'):\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name} ({gpu_mem:.2f} GB)\")\n    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))\n    print(f\"Dynamic batch size = {BATCH_SIZE} (free ~{free_mem:.2f} GB)\")\nelse:\n    print(\"GPU not available, using CPU\")\n    BATCH_SIZE = 4\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2 — Utilities (profiling, IO helpers, normalization)\n","metadata":{}},{"cell_type":"code","source":"# Part 2 — Utilities (profiling, IO helpers, normalization)\n# =========================================\n\nclass GPUProfiler:\n    \"\"\"Simple context manager to profile CUDA/CPU sections.\"\"\"\n    def __init__(self, name: str):\n        self.name = name\n        self.start_time = None\n    def __enter__(self):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.start_time = time.time()\n        return self\n    def __exit__(self, *args):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        elapsed = time.time() - self.start_time\n        print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n\ndef normalize_slice(slice_data: np.ndarray) -> np.ndarray:\n    \"\"\"Contrast-stretch using 2nd–98th percentiles, returning uint8 [0..255].\"\"\"\n    p2, p98 = np.percentile(slice_data, [2, 98])\n    clipped = np.clip(slice_data, p2, p98)\n    norm = 255 * (clipped - p2) / max(1e-6, (p98 - p2))\n    return np.uint8(norm)\n\ndef preload_image_batch(file_paths):\n    \"\"\"Preload a batch of images into CPU memory (unused with TTA per-image inference).\"\"\"\n    images = []\n    for path in file_paths:\n        img = cv2.imread(path)\n        if img is None:\n            img = np.array(Image.open(path))\n        images.append(img)\n    return images\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 3 — TTA Inference (run YOLO with flips & map boxes back)\n","metadata":{}},{"cell_type":"code","source":"def _apply_tta(image_bgr: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"Return augmented image for a given TTA mode.\"\"\"\n    if mode == 'orig':\n        return image_bgr\n    if mode == 'hflip':\n        return cv2.flip(image_bgr, 1)\n    if mode == 'vflip':\n        return cv2.flip(image_bgr, 0)\n    raise ValueError(f\"Unsupported TTA mode: {mode}\")\n\ndef _map_back_xyxy(xyxy: np.ndarray, mode: str, w: int, h: int) -> np.ndarray:\n    \"\"\"\n    Map [x1, y1, x2, y2] from TTA'ed image back to original orientation.\n    \"\"\"\n    x1, y1, x2, y2 = xyxy\n    if mode == 'orig':\n        return np.array([x1, y1, x2, y2], dtype=np.float32)\n    if mode == 'hflip':\n        # x' = w-1-x ; swap after flipping box ends\n        nx1, nx2 = (w - 1 - x2), (w - 1 - x1)\n        return np.array([nx1, y1, nx2, y2], dtype=np.float32)\n    if mode == 'vflip':\n        ny1, ny2 = (h - 1 - y2), (h - 1 - y1)\n        return np.array([x1, ny1, x2, ny2], dtype=np.float32)\n    raise ValueError(f\"Unsupported TTA mode: {mode}\")\n\ndef infer_image_with_tta(model, img_path: str, conf_thr: float):\n    \"\"\"\n    Run YOLO on a single image with TTA and return a list of detections\n    mapped back to the original orientation. Each detection is a dict:\n    {'x': center_x, 'y': center_y, 'confidence': float}\n    \"\"\"\n    # Load BGR image (3-ch). YOLO expects color images.\n    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    if img is None:\n        # Fall back to PIL if needed\n        img = cv2.cvtColor(np.array(Image.open(img_path)), cv2.COLOR_RGB2BGR)\n    h, w = img.shape[:2]\n\n    all_boxes = []\n    all_confs = []\n\n    modes = TTA_MODES if ENABLE_TTA else ['orig']\n    for mode in modes:\n        aug = _apply_tta(img, mode)\n\n        # Run model on numpy image (Ultralytics supports np arrays)\n        results = model([aug], verbose=False)\n        res = results[0]\n        if res.boxes is None or len(res.boxes) == 0:\n            continue\n\n        # Map boxes back to original orientation\n        for bi in range(len(res.boxes)):\n            conf = float(res.boxes.conf[bi])\n            if conf < conf_thr:\n                continue\n            xyxy = res.boxes.xyxy[bi].cpu().numpy()\n            xyxy_back = _map_back_xyxy(xyxy, mode, w, h)\n            all_boxes.append(xyxy_back)\n            all_confs.append(conf)\n\n    detections = []\n    for xyxy, conf in zip(all_boxes, all_confs):\n        x1, y1, x2, y2 = xyxy\n        cx = (x1 + x2) / 2.0\n        cy = (y1 + y2) / 2.0\n        detections.append({'x': float(cx), 'y': float(cy), 'confidence': float(conf)})\n\n    return detections","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 4 — 3D NMS & Per-Tomogram Processing\n","metadata":{}},{"cell_type":"code","source":"def perform_3d_nms(detections, iou_threshold: float):\n    \"\"\"\n    Lightweight 3D NMS using a distance threshold based on an approximate box size.\n    Detections are dicts with keys: 'z','y','x','confidence'.\n    \"\"\"\n    if not detections:\n        return []\n\n    detections = sorted(detections, key=lambda d: d['confidence'], reverse=True)\n    final_dets = []\n\n    def distance_3d(d1, d2):\n        return np.sqrt((d1['z'] - d2['z'])**2 + (d1['y'] - d2['y'])**2 + (d1['x'] - d2['x'])**2)\n\n    # Approximate lateral box size (pixels); threshold is scaled by IOU-like factor.\n    box_size = 24\n    dist_thr = box_size * iou_threshold\n\n    while detections:\n        best = detections.pop(0)\n        final_dets.append(best)\n        detections = [d for d in detections if distance_3d(d, best) > dist_thr]\n\n    return final_dets\n\ndef process_tomogram(tomo_id: str, model, index=0, total=1):\n    \"\"\"\n    Process a single tomogram (folder of slices). Uses per-image TTA.\n    Returns best detection mapped to submission format.\n    \"\"\"\n    print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n    tomo_dir = os.path.join(test_dir, tomo_id)\n    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n\n    # Subsample by CONCENTRATION (for quick submissions)\n    select_idx = np.linspace(0, len(slice_files) - 1, max(1, int(len(slice_files) * CONCENTRATION)))\n    select_idx = np.round(select_idx).astype(int)\n    slice_files = [slice_files[i] for i in select_idx]\n    print(f\"Using {len(slice_files)} / {len(os.listdir(tomo_dir))} slices (CONCENTRATION={CONCENTRATION})\")\n\n    all_detections = []\n\n    # Streams kept for parity; TTA runs per image (loop below).\n    streams = [torch.cuda.Stream() for _ in range(min(4, BATCH_SIZE))] if device.startswith('cuda') else [None]\n\n    # Iterate slices; per-slice TTA inference\n    for k, slice_file in enumerate(slice_files):\n        stream = streams[k % len(streams)]\n        with torch.cuda.stream(stream) if (stream and device.startswith('cuda')) else nullcontext():\n            img_path = os.path.join(tomo_dir, slice_file)\n            slice_num = int(slice_file.split('_')[1].split('.')[0])\n\n            with GPUProfiler(f\"Inference (TTA) slice {k+1}/{len(slice_files)}\"):\n                dets = infer_image_with_tta(model, img_path, CONFIDENCE_THRESHOLD)\n\n            # Collect detections with z index\n            for d in dets:\n                all_detections.append({\n                    'z': round(slice_num),\n                    'y': round(d['y']),\n                    'x': round(d['x']),\n                    'confidence': float(d['confidence'])\n                })\n\n    if device.startswith('cuda'):\n        torch.cuda.synchronize()\n\n    # 3D NMS across the whole volume\n    final_dets = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n    final_dets.sort(key=lambda x: x['confidence'], reverse=True)\n\n    if not final_dets:\n        return {'tomo_id': tomo_id, 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n\n    best = final_dets[0]\n    return {\n        'tomo_id': tomo_id,\n        'Motor axis 0': int(best['z']),\n        'Motor axis 1': int(best['y']),\n        'Motor axis 2': int(best['x']),\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 5 — Debug helpers & Submission generation\n","metadata":{}},{"cell_type":"code","source":"def debug_image_loading(tomo_id: str):\n    \"\"\"Quick check: can we read images with PIL/CV2 and run the model on one frame.\"\"\"\n    tomo_dir = os.path.join(test_dir, tomo_id)\n    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n    if not slice_files:\n        print(f\"No image files found in {tomo_dir}\")\n        return\n\n    print(f\"Found {len(slice_files)} image files in {tomo_dir}\")\n    sample = slice_files[len(slice_files) // 2]\n    img_path = os.path.join(tomo_dir, sample)\n\n    try:\n        img_pil = Image.open(img_path)\n        print(f\"PIL shape={np.array(img_pil).shape}, dtype={np.array(img_pil).dtype}\")\n        img_cv2 = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        print(f\"CV2 gray shape={img_cv2.shape}, dtype={img_cv2.dtype}\")\n        img_rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        print(f\"CV2 RGB shape={img_rgb.shape}, dtype={img_rgb.dtype}\")\n        print(\"Image loading OK.\")\n    except Exception as e:\n        print(f\"Image loading error: {e}\")\n\n    try:\n        test_model = YOLO(model_path)\n        _ = test_model([cv2.imread(img_path)], verbose=False)\n        print(\"YOLO processed the test image OK.\")\n    except Exception as e:\n        print(f\"YOLO test error: {e}\")\n\ndef generate_submission():\n    \"\"\"\n    Orchestrate end-to-end inference over all tomograms and save submission.csv.\n    \"\"\"\n    test_tomos = sorted([d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))])\n    total = len(test_tomos)\n    print(f\"Found {total} tomograms in: {test_dir}\")\n\n    if total > 0:\n        debug_image_loading(test_tomos[0])\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    print(f\"Loading YOLO model from: {model_path}\")\n    model = YOLO(model_path)\n    model.to(device)\n\n    # Optional speed/precision tweaks\n    if device.startswith('cuda'):\n        try:\n            model.fuse()\n        except Exception:\n            pass\n        if torch.cuda.get_device_capability(0)[0] >= 7:\n            try:\n                model.model.half()\n                print(\"Using half precision (FP16) for inference\")\n            except Exception:\n                print(\"FP16 not applied (model backend may not support).\")\n\n    results = []\n    motors_found = 0\n\n    # Simple single-thread orchestration (safe with per-image TTA)\n    for i, tomo_id in enumerate(test_tomos, 1):\n        try:\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            out = process_tomogram(tomo_id, model, i, total)\n            results.append(out)\n\n            # In this pipeline, -1 indicates \"no detection\"\n            has_motor = (out['Motor axis 0'] >= 0)\n            if has_motor:\n                motors_found += 1\n                print(f\"Motor in {tomo_id}: z={out['Motor axis 0']}, y={out['Motor axis 1']}, x={out['Motor axis 2']}\")\n            else:\n                print(f\"No motor detected in {tomo_id}\")\n\n            rate = 100.0 * motors_found / len(results)\n            print(f\"Current detection rate: {motors_found}/{len(results)} ({rate:.1f}%)\")\n\n        except Exception as e:\n            print(f\"Error processing {tomo_id}: {e}\")\n            results.append({'tomo_id': tomo_id, 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1})\n\n    # Save submission\n    submission_df = pd.DataFrame(results)[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\n    submission_df.to_csv(submission_path, index=False)\n\n    print(\"\\nSubmission complete!\")\n    print(f\"Motors detected: {motors_found}/{total} ({(100.0*motors_found/max(1,total)):.1f}%)\")\n    print(f\"Saved to: {submission_path}\")\n    print(\"\\nPreview:\")\n    print(submission_df.head())\n    return submission_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 6 — Main\n","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    start = time.time()\n    submission = generate_submission()\n    elapsed = time.time() - start\n    print(f\"\\nTotal execution time: {elapsed:.2f} s ({elapsed/60:.2f} min)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}